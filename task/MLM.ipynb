{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLTrKIIuDfd4","executionInfo":{"status":"ok","timestamp":1714288262509,"user_tz":420,"elapsed":21137,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"28fa1fe3-3383-451e-dcbf-80c2c91410ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/CS598_PROJECT')"],"metadata":{"id":"EvE83jdaDy7G","executionInfo":{"status":"ok","timestamp":1714288264345,"user_tz":420,"elapsed":163,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["pip install pytorch_pretrained_bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phPMLIUVEifm","executionInfo":{"status":"ok","timestamp":1714288238322,"user_tz":420,"elapsed":114226,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"4a57f709-2c8e-4ba7-d878-2d5e08c0eb73"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m92.2/123.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.25.2)\n","Collecting boto3 (from pytorch_pretrained_bert)\n","  Downloading boto3-1.34.93-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.12.25)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Collecting botocore<1.35.0,>=1.34.93 (from boto3->pytorch_pretrained_bert)\n","  Downloading botocore-1.34.93-py3-none-any.whl (12.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert)\n","  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.2.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.93->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.93->boto3->pytorch_pretrained_bert) (1.16.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert\n","Successfully installed boto3-1.34.93 botocore-1.34.93 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.1\n","^C\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8oktrfDQDfd6","executionInfo":{"status":"ok","timestamp":1714288276049,"user_tz":420,"elapsed":8919,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["from common.common import create_folder\n","from common.pytorch import load_model\n","import pytorch_pretrained_bert as Bert\n","from model.utils import age_vocab\n","from common.common import load_obj\n","from dataLoader.MLM import MLMLoader\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","from model.MLM import BertForMaskedLM\n","from model.optimiser import adam\n","import sklearn.metrics as skm\n","import numpy as np\n","import torch\n","import time\n","import torch.nn as nn\n","import os"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ke8DBjvcDfd6","executionInfo":{"status":"ok","timestamp":1714288278859,"user_tz":420,"elapsed":143,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["class BertConfig(Bert.modeling.BertConfig):\n","    def __init__(self, config):\n","        super(BertConfig, self).__init__(\n","            vocab_size_or_config_json_file=config.get('vocab_size'),\n","            hidden_size=config['hidden_size'],\n","            num_hidden_layers=config.get('num_hidden_layers'),\n","            num_attention_heads=config.get('num_attention_heads'),\n","            intermediate_size=config.get('intermediate_size'),\n","            hidden_act=config.get('hidden_act'),\n","            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n","            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n","            max_position_embeddings = config.get('max_position_embedding'),\n","            initializer_range=config.get('initializer_range'),\n","        )\n","        self.seg_vocab_size = config.get('seg_vocab_size')\n","        self.age_vocab_size = config.get('age_vocab_size')\n","\n","class TrainConfig(object):\n","    def __init__(self, config):\n","        self.batch_size = config.get('batch_size')\n","        self.use_cuda = config.get('use_cuda')\n","        self.max_len_seq = config.get('max_len_seq')\n","        self.train_loader_workers = config.get('train_loader_workers')\n","        self.test_loader_workers = config.get('test_loader_workers')\n","        self.device = config.get('device')\n","        self.output_dir = config.get('output_dir')\n","        self.output_name = config.get('output_name')\n","        self.best_name = config.get('best_name')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SBXJ-ouqDfd7","executionInfo":{"status":"ok","timestamp":1714288280130,"user_tz":420,"elapsed":380,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["file_config = {\n","    'vocab':'/content/drive/MyDrive/CS598_PROJECT/output/token2idx',  # vocabulary idx2token, token2idx\n","    'data': '/content/drive/MyDrive/CS598_PROJECT/output/dataset.pkl',  # formated data\n","    'model_path': '/content/drive/MyDrive/CS598_PROJECT/modeloutput', # where to save model\n","    'model_name': 'MLM_MODEL', # model name\n","    'file_name': 'MLM_LOG',  # log path\n","}\n","create_folder(file_config['model_path'])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HnPK5I15Dfd7","executionInfo":{"status":"ok","timestamp":1714288281292,"user_tz":420,"elapsed":213,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["global_params = {\n","    'max_seq_len': 64,\n","    'max_age': 110,\n","    'month': 1,\n","    'age_symbol': None,\n","    'min_visit': 3,\n","    'gradient_accumulation_steps': 1\n","}\n","\n","optim_param = {\n","    'lr': 3e-5,\n","    'warmup_proportion': 0.1,\n","    'weight_decay': 0.01\n","}\n","\n","train_params = {\n","    'batch_size': 256,\n","    'use_cuda': True,\n","    'max_len_seq': global_params['max_seq_len'],\n","    #'device': 'cuda:0'\n","    'device': 'cpu'\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dA_G0qlNDfd8","executionInfo":{"status":"ok","timestamp":1714288282644,"user_tz":420,"elapsed":367,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["BertVocab = load_obj(file_config['vocab'])\n","ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"]},{"cell_type":"code","source":["data = pd.read_pickle(file_config['data'])\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmW4hiiCUvCI","executionInfo":{"status":"ok","timestamp":1714288283922,"user_tz":420,"elapsed":388,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"b8e52caf-eb4f-48d5-c21f-ba5a06d1cfc7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(7532, 3)\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"UBa6xyscDfd8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714288285121,"user_tz":420,"elapsed":2,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"2e847013-42cb-47b5-d9cc-9dd6de88bd07"},"outputs":[{"output_type":"stream","name":"stdout","text":["(2374, 4)\n"]}],"source":["# data = pd.read_parquet(file_config['data'])\n","# remove patients with visits less than min visit\n","data['length'] = data['ICD9_CODE'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n","data = data[data['length'] >= global_params['min_visit']]\n","data = data.reset_index(drop=True)\n","print(data.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"W_9pBx_iDfd8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714288286027,"user_tz":420,"elapsed":2,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"581afd34-c0f8-45ab-e7c2-6221f09026ae"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["Dset = MLMLoader(data, BertVocab['token2idx'], ageVocab, max_len=train_params['max_len_seq'], code='ICD9_CODE', age = 'AGE')\n","trainload = DataLoader(dataset=Dset, batch_size=train_params['batch_size'], shuffle=True, num_workers=3)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"hFNG-kb9Dfd9","executionInfo":{"status":"ok","timestamp":1714288287209,"user_tz":420,"elapsed":1,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["model_config = {\n","    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n","    'hidden_size': 288, # word embedding and seg embedding hidden size\n","    'seg_vocab_size': 2, # number of vocab for seg embedding\n","    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n","    'max_position_embedding': train_params['max_len_seq'], # maximum number of tokens\n","    'hidden_dropout_prob': 0.1, # dropout rate\n","    'num_hidden_layers': 6, # number of multi-head attention layers required\n","    'num_attention_heads': 12, # number of attention heads\n","    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n","    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n","    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n","    'initializer_range': 0.02, # parameter weight initializer range\n","}"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"CWdPd8khDfd9","executionInfo":{"status":"ok","timestamp":1714288288804,"user_tz":420,"elapsed":465,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["conf = BertConfig(model_config)\n","model = BertForMaskedLM(conf)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"hAinb6ycDfd9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714288293457,"user_tz":420,"elapsed":3308,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}},"outputId":"54eb1b56-1fff-4113-850f-11020cc77d16"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"]}],"source":["model = model.to(train_params['device'])\n","optim = adam(params=list(model.named_parameters()), config=optim_param)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"NwPUmxgxDfd9","executionInfo":{"status":"ok","timestamp":1714288294858,"user_tz":420,"elapsed":256,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["def cal_acc(label, pred):\n","    logs = nn.LogSoftmax()\n","    label=label.cpu().numpy()\n","    ind = np.where(label!=-1)[0]\n","    truepred = pred.detach().cpu().numpy()\n","    truepred = truepred[ind]\n","    truelabel = label[ind]\n","    truepred = logs(torch.tensor(truepred))\n","    outs = [np.argmax(pred_x) for pred_x in truepred.numpy()]\n","    precision = skm.precision_score(truelabel, outs, average='micro')\n","    return precision"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"_oRCbJXbDfd-","executionInfo":{"status":"ok","timestamp":1714288296177,"user_tz":420,"elapsed":2,"user":{"displayName":"Bo Hu","userId":"00463525530088648348"}}},"outputs":[],"source":["\n","def train(e, loader):\n","    tr_loss = 0\n","    temp_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    cnt= 0\n","    start = time.time()\n","\n","    for step, batch in enumerate(loader):\n","        cnt +=1\n","        batch = tuple(t.to(train_params['device']) for t in batch)\n","        age_ids, input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n","        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, masked_lm_labels=masked_label)\n","        if global_params['gradient_accumulation_steps'] >1:\n","            loss = loss/global_params['gradient_accumulation_steps']\n","        loss.backward()\n","\n","        temp_loss += loss.item()\n","        tr_loss += loss.item()\n","\n","        nb_tr_examples += input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","        if step % 200==0:\n","            print(\"epoch: {}\\t| cnt: {}\\t|Loss: {}\\t| precision: {:.4f}\\t| time: {:.2f}\".format(e, cnt, temp_loss/2000, cal_acc(label, pred), time.time()-start))\n","            temp_loss = 0\n","            start = time.time()\n","\n","        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n","            optim.step()\n","            optim.zero_grad()\n","\n","    print(\"** ** * Saving fine - tuned model ** ** * \")\n","    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","    create_folder(file_config['model_path'])\n","    output_model_file = os.path.join(file_config['model_path'], file_config['model_name'])\n","\n","    torch.save(model_to_save.state_dict(), output_model_file)\n","\n","    cost = time.time() - start\n","    return tr_loss, cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooVLcm9tDfd-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23c8a19c-1122-4d71-a6ad-08f0eeb6eea6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n","/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0\t| cnt: 1\t|Loss: 0.0034992244243621825\t| precision: 0.0022\t| time: 23.60\n","** ** * Saving fine - tuned model ** ** * \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]}],"source":["f = open(os.path.join(file_config['model_path'], file_config['file_name']), \"w\")\n","f.write('{}\\t{}\\t{}\\n'.format('epoch', 'loss', 'time'))\n","for e in range(50):\n","    loss, time_cost = train(e, trainload)\n","    loss = loss/data.shape[0]\n","    f.write('{}\\t{}\\t{}\\n'.format(e, loss, time_cost))\n","f.close()"]},{"cell_type":"code","source":[],"metadata":{"id":"jxg1cC9rZ5oG"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}